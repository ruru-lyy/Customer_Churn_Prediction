{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4243e58b",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction. \n",
    "\n",
    "## Introduction\n",
    "Customer churn is the rate at which customers stop doing business with a company. It a critical metric across industries, especially in subscription-based models like telecom, SaaS, and media. Studies show that acquiring a new customer costs 5–7 times more than retaining an existing one, and a 5% increase in retention can boost profits by 25% to 95% (Harvard Business Review).\n",
    "\n",
    "In this context, data analytics and predictive modeling are indispensable tools. They help identify patterns in customer behavior, detect early warning signs, and enable targeted interventions to reduce churn. By leveraging features like Tenure, Usage Frequency, Payment Delays, and Support Interactions, machine learning models can forecast churn with high accuracy empowering businesses to act before it's too late."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0da86",
   "metadata": {},
   "source": [
    "## 1. Data Understanding & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the aesthetics\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\", \"#9b59b6\"]\n",
    "sns.set_palette(sns.color_palette(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # for reproducinbility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848fa469",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\\\datasets\\\\customer_churn_dataset-training-master.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd267be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009edc4",
   "metadata": {},
   "source": [
    "The dataset has 400k+ records and 12 cols.\n",
    "\n",
    "\n",
    "- **CustomerID**: Unique identifier for each customer (e.g., 2, 3, 4…).\n",
    "- **Age**: Customer’s age in years (e.g., 30, 65).\n",
    "- **Gender**: Customer’s gender (`Male`, `Female`).\n",
    "- **Tenure**: Number of months the customer has stayed subscribed (e.g., 14, 39).\n",
    "- **Usage Frequency**: Average number of service uses per month (e.g., 1 to 25).\n",
    "- **Support Calls**: Number of support/helpdesk calls made (e.g., 0 to 10+).\n",
    "- **Payment Delay**: Average days of delayed payments (e.g., 0 to 30+).\n",
    "- **Subscription Type**: Type of plan (`Basic`, `Standard`, `Premium`).\n",
    "- **Contract Length**: Billing cycle (`Monthly`, `Quarterly`, `Annual`).\n",
    "- **Total Spend**: Total amount spent by the customer till date (e.g., ₹185 to ₹932).\n",
    "- **Last Interaction**: Days since last customer engagement (e.g., 3 to 29).\n",
    "- **Churn**: Target variable indicating if customer left (1 = Churned, 0 = Active).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff6344",
   "metadata": {},
   "source": [
    "Since missing values are significantly less in number (less than 0.5%), they can be dropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6666a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c507572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c1e58",
   "metadata": {},
   "source": [
    "## Checking for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d78611",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Age', 'Tenure', 'Usage Frequency', 'Support Calls', 'Payment Delay', 'Total Spend', 'Last Interaction']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/outliers_boxplot.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebef63a",
   "metadata": {},
   "source": [
    "No outliers detected. Data cleaning is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1bc982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb2abd",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "Correlation analysis reveals linear relationships between variables helping us understand which features may influence churn and which ones move together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fcf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df[numeric_cols + ['Churn']].corr()\n",
    "print(correlation_matrix['Churn'].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfebdc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.savefig('../outputs/correlation_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ef00e",
   "metadata": {},
   "source": [
    "**Observations and inisghts from the correlation analysis**\n",
    "\n",
    "| Feature          | Correlation with churn | Observation                                                                 | Insight                                                                                                                                | Action                                                                                                                                                                                                                                                                                          |\n",
    "|------------------|-------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Support Calls** | **0.57 (Positive)** | Higher number of support calls is associated with a higher likelihood of churn. | Customers requiring frequent support might be experiencing issues or dissatisfaction with the product/service. | Investigate call reasons why they're happening. Call first to check up on issues.                                                                                                                                                                                                                          |\n",
    "| **Payment Delay** | **0.31 (Positive)** | Longer or more frequent payment delays are linked to a higher chance of churn. | Customers delaying payments might be financially strained or overthinking the quality/ ROI                                       | Communicate about payment offers/discounts options, our target is retention.                                                                                                                                                                                                                                  |\n",
    "| **Age** | **0.22 (Positive)** | Older customers show a slightly higher tendency to churn.                   | Age might be a factor influencing churn, potentially due to changing needs, technology adoption, or retirement.                                                              | Analyze needs of older demographic, tailor communication/support.                                                                                                                                                                                                                         |\n",
    "| **Last Interaction**| **0.15 (Positive)** | More recent last interaction is weakly associated with a slightly higher churn. | might suggest that recent interactions were negative or didn't resolve underlying issues leading to churn shortly after.                       | Analyze recent calls, dont leave issues unresolved                                                                                                                                                                                                                                |\n",
    "| **Tenure** | **-0.05 (Negative)** | Longer tenure has a very weak negative association with churn.              | While weak, it suggests that customers who have been with the company longer are slightly less likely to churn. Building long-term relationships is generally beneficial.          | Focus on long-term loyalty programs                                                                                                                                                                                                                                           |\n",
    "| **Usage Frequency**| **-0.05 (Negative)** | Higher usage frequency has a very weak negative association with churn.     | Simply using the service more often doesn't strongly guarantee retention.                                         | Investigate quality/purpose of usage,focus on value delivery.                                                                                                                                                                                                                                      |\n",
    "| **Total Spend** | **-0.43 (Negative)** | Higher total spending is moderately associated with a lower likelihood of churn. | Customers who have invested more financially are less likely to leave, possibly due to initial high investment. This is a valuable segment to retain.         | Prioritize high-spending customers, offer premium support/rewards.                                                                                                                                                                                                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9e678",
   "metadata": {},
   "source": [
    "For further analysis of relationships between the variables, we shall perform statistical tests like `Chi-square` test and `ANOVA` test. Correlation analysis only captures linear relationships between numerical variables. It does not handle categorical variables well or detect non-linear dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d999bd",
   "metadata": {},
   "source": [
    "## Chi-square tests for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['Gender', 'Subscription Type', 'Contract Length']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    contingency_table = pd.crosstab(df[col], df['Churn'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    print(f\"{col}: Chi2 = {chi2:.2f}, p-value = {p:.10f}\")\n",
    "    if p < 0.05:\n",
    "        print(f\"  - Significant association detected with churn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe3022",
   "metadata": {},
   "source": [
    "When analyzing relationships between categorical variables using tests like the Chi-Squared test for independence, a reported p-value of 0 still means extremely strong evidence against the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abe79d",
   "metadata": {},
   "source": [
    "## ANOVA Tests for numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df699cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7348ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in numeric_cols:\n",
    "    churned = df[df['Churn'] == 1][col]\n",
    "    not_churned = df[df['Churn'] == 0][col]\n",
    "    f_stat, p_value = f_oneway(churned, not_churned)\n",
    "    print(f\"{col}: F = {f_stat:.2f}, p-value = {p_value:.10f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"  - Significant difference between churned and non-churned customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e45556",
   "metadata": {},
   "source": [
    "A reported p-value of 0 in ANOVA means we have very strong evidence to reject the null hypothesis that all group means are equal.Since it is theoretically impossible for p-value to be exact 0, reported p-value of 0 could mean that it is smaller than the level of precision the software can display so report the p-value as being less than a very small threshold (e.g., p < 0.001).\n",
    "\n",
    "What these two test results mean is such low p-values mean the patterns you're seeing are real, not random noise ideal targets for retention strategy or model features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e37656",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63599e",
   "metadata": {},
   "source": [
    "### What is the overall churn rate in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_rate = df['Churn'].mean() * 100\n",
    "print(f\"Overall churn rate: {churn_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='Churn', data=df, palette=['#3498db', '#e74c3c'])\n",
    "plt.title('Churn Distribution')\n",
    "plt.xlabel('Churn (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "total = len(df)\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2., height + 5000,\n",
    "            f'{height} ({height/total*100:.1f}%)',\n",
    "            ha=\"center\", fontsize=12)\n",
    "\n",
    "plt.savefig('../outputs/churn_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c311ac54",
   "metadata": {},
   "source": [
    "### Numerical variable distribtuions by churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cf820",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # hist with density plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data=df, x=col, hue='Churn', kde=True, element='step')\n",
    "    plt.title(f'Distribution of {col} by Churn')\n",
    "    \n",
    "    # violin plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.violinplot(x='Churn', y=col, data=df, inner='quartile')\n",
    "    plt.title(f'Violin Plot of {col} by Churn')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../outputs/distribution_{col}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceeb442",
   "metadata": {},
   "source": [
    "### Categorical variable analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4651bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    churn_pct = df.groupby(col)['Churn'].value_counts(normalize=True).unstack().fillna(0) * 100\n",
    "    churn_pct = churn_pct[[0, 1]] \n",
    "\n",
    "    churn_pct.plot(kind='bar', stacked=True, color=colors, ax=plt.gca())\n",
    "\n",
    "    plt.title(f'{col} - Churn Distribution (%)')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Churn', labels=['No', 'Yes'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../outputs/stackedbar_{col}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f026ea",
   "metadata": {},
   "source": [
    "### Cross-tabulation analysis: Contract Length vs Subscription Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dac6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cross_tab = pd.crosstab(df['Contract Length'], df['Subscription Type'], \n",
    "                         values=df['Churn'], aggfunc='mean') * 100\n",
    "print(cross_tab)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cross_tab, annot=True, cmap='YlOrRd', fmt='.1f')\n",
    "plt.title('Churn Rate (%) by Contract Length and Subscription Type')\n",
    "plt.savefig('../outputs/churn_heatmap_contract_subscription.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0470708",
   "metadata": {},
   "source": [
    "Customers on Monthly contracts have a 100% churn rate across all subscription types!! making them the most unstable segment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79189d0",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Let us introduce some new features to turn raw data into behavioral signals making the model smarter at detecting early signs of churn.\n",
    "\n",
    "\n",
    "1. `CLV` (customer lifetime value)  \n",
    "   tells you how much money a customer might bring in over time. low clv = kinda low value, probably gonna churn soon.\n",
    "\n",
    "2. `monthly_spend = total spend / tenure`  \n",
    "   shows how regularly they spend. if it’s low or suddenly drops, they might be losing interest or too price-sensitive.\n",
    "\n",
    "3. `usage_intensity = usage freq / tenure`  \n",
    "   gives an idea of how *into* the product they are. low usage even after staying long = maybe bored or not finding value.\n",
    "\n",
    "4. `support_call_rate = support calls / tenure`  \n",
    "   too many calls? probably had issues. if issues doesn’t get fixed, they quit.\n",
    "\n",
    "5. `last_interaction_ratio = last interaction / tenure`  \n",
    "   basically checks if they stopped interaction recently. high ratio = they used to be active. pre-churn alert.\n",
    "\n",
    "6. `payment_delay_tenure_ratio = payment delay / tenure`  \n",
    "   if they keep delaying payments often, it’s a sign they'll churn. either money issues or just not invested anymore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800159dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = ['CLV', 'Monthly_Spend', 'Usage_Intensity', 'Support_Call_Rate', \n",
    "                'Last_Interaction_Ratio', 'Payment_Delay_Tenure_Ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09378498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CLV'] = df['Total Spend'] / (df['Tenure'] + 1)  # add 1 to avoid division by zero\n",
    "df['Monthly_Spend'] = df['Total Spend'] / (df['Tenure'] + 1)\n",
    "df['Usage_Intensity'] = df['Usage Frequency'] / (df['Tenure'] + 1)\n",
    "df['Support_Call_Rate'] = df['Support Calls'] / (df['Tenure'] + 1)\n",
    "df['Last_Interaction_Ratio'] = df['Last Interaction'] / (df['Tenure'] + 1)\n",
    "df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 35, 50, 65, 100], \n",
    "                         labels=['Under 25', '25-35', '36-50', '51-65', 'Over 65'])\n",
    "df['Tenure_Group'] = pd.cut(df['Tenure'], bins=[0, 6, 12, 24, 36, float('inf')], \n",
    "                           labels=['0-6 Months', '7-12 Months', '1-2 Years', '2-3 Years', '3+ Years'])\n",
    "df['Payment_Delay_Tenure_Ratio'] = df['Payment Delay'] / (df['Tenure'] + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa92a9c",
   "metadata": {},
   "source": [
    "### Correlation of new features with churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in new_features:\n",
    "    correlation = df[feature].corr(df['Churn'])\n",
    "    print(f\"{feature}: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f7422",
   "metadata": {},
   "source": [
    "Lets store these as our final features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514085a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    'Age', 'Tenure', 'Usage Frequency', 'Support Calls', 'Payment Delay',\n",
    "    'Total Spend', 'Last Interaction', 'CLV', 'Monthly_Spend',\n",
    "    'Usage_Intensity', 'Support_Call_Rate', 'Last_Interaction_Ratio',\n",
    "    'Payment_Delay_Tenure_Ratio'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    'Gender', 'Subscription Type', 'Contract Length',\n",
    "    'Age_Group', 'Tenure_Group'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce1af6",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2974ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numerical features: {numerical_cols}\")\n",
    "print(f\"Categorical features: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8406ed",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8503aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a custom label encoder transformer\n",
    "def label_encode_columns(X):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import pandas as pd\n",
    "    X = X.copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        X.iloc[:, i] = le.fit_transform(X.iloc[:, i])\n",
    "    return X\n",
    "\n",
    "label_encoder = FunctionTransformer(label_encode_columns, validate=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', label_encoder, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa51dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(preprocessor, \"../vault/churn_preprocessor.pkl\")\n",
    "print(\"Saved: churn_preprocessor.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a08853",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d9ea2",
   "metadata": {},
   "source": [
    "The class distribution is fairly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd69dd7",
   "metadata": {},
   "source": [
    "### Train and evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd2949",
   "metadata": {},
   "source": [
    "\n",
    "###  why **Logistic Regression**, **Random Forest**, & **XGBoost**?\n",
    "\n",
    "\n",
    "#### 1. logistic regression(interpretable baseline)  \n",
    "- a linear classifier that estimates log-odds of churn, outputting calibrated probabilities.  \n",
    "- great for feature coefficient analysis making it valuable for insights on individual predictor impact.  \n",
    "- highly interpretable, minimal preprocessing, and computationally efficient perfect for early-stage exploration or explainability-heavy contexts.\n",
    "\n",
    "#### 2. random forest (ensemble generalist)  \n",
    "- non-parametric model that builds multiple decision trees and aggregates outputs (bagging).  \n",
    "- captures nonlinear relationship and complex feature interactions without overfitting (thanks to randomness).  \n",
    "- robust to outliers, handles both numerical and categorical features well.  \n",
    "- provides feature importance scores, enabling model explainability while still achieving strong performance.\n",
    "\n",
    "\n",
    "#### 3. xgboost (gradient boosting powerhouse)  \n",
    "- optimized implementation of gradient boosted trees, known for regularized learning and sparse-aware algorithms.  \n",
    "- strong generalization via shrinkage, column subsampling and tree pruning.  .  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9565a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report, \n",
    "    roc_curve, precision_recall_curve, ConfusionMatrixDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = []\n",
    "best_auc = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    y_pred_proba = model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        top_models = [(name, model)]\n",
    "    elif auc == best_auc:\n",
    "        top_models.append((name, model))\n",
    "\n",
    "# Save all models with best AUC\n",
    "for name, model in top_models:\n",
    "    model_path = f\"../vault/{name.replace(' ', '_').lower()}_model.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c1f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Random Forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09baba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['XGBoost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61552648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'Accuracy': vals['accuracy'],\n",
    "        'Precision': vals['precision'],\n",
    "        'Recall': vals['recall'],\n",
    "        'F1-Score': vals['f1'],\n",
    "        'ROC AUC': vals['auc']\n",
    "    }\n",
    "    for name, vals in results.items()\n",
    "}).T\n",
    "\n",
    "metrics_df = metrics_df.sort_values(by=['F1-Score'], ascending=False)\n",
    "\n",
    "\n",
    "print(\"\\n Model Comparison Metrics:\")\n",
    "print(metrics_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "xgb_model = results['XGBoost']['model']\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': final_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp_df.head(10), palette='mako')\n",
    "plt.title('Top XGBoost Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73f1d5",
   "metadata": {},
   "source": [
    "### SHAP VALUES for Model Explainability\n",
    "\n",
    "SHAP values (SHapley Additive exPlanations) quantify how much each feature contributes to a specific prediction, based on game theory.\n",
    "\n",
    "A SHAP summary plot shows overall feature importance and how each feature’s value (high/low) impacts model output across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73fbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(results['XGBoost']['model'])\n",
    "\n",
    "shap_values = explainer.shap_values(X_train_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9535db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_processed, feature_names=final_feature_names, show=False)\n",
    "\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(12, 10)\n",
    "fig.savefig('../outputs/shap_summary.png', bbox_inches='tight')\n",
    "plt.show(fig)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287c3b9",
   "metadata": {},
   "source": [
    "## SHAP summary - Conclusion \n",
    "\n",
    "- Each dot represents a single customer. Where the dot sits on the horizontal axis tells us how much that specific feature pushed the model's prediction for that customer towards either \"more likely to churn\" (positive SHAP value, leaning towards the right) or \"less likely to churn\" (negative SHAP value, leaning left).\n",
    "\n",
    "- The color of the dot is also important. It tells us the original value of that feature for that customer. Reddish colors usually mean a higher value for that feature, while bluish colors mean a lower value.\n",
    "\n",
    "For example, peep at \"Support Calls.\" You can see a bunch of reddish dots way over on the right side. This strongly suggests that customers who make a lot of support calls are much more likely to churn, according to our model. On the flip side, blue dots for \"Support Calls\" are mostly on the left, meaning fewer support calls make someone less likely to leave.\n",
    "\n",
    "- Take a look at \"Tenure\" down there. You'll notice a lot of red dots hanging out on the left side. This implies that customers with a longer tenure (been with us for a while) are generally less likely to churn. The blue dots (shorter tenure) are more spread out, some even venturing towards the right, indicating that newer customers have a more varied impact on churn prediction.\n",
    "\n",
    "By looking at the distribution and color of these dots, we can get a pretty good idea of how different customer characteristics influence their likelihood of churning. It's super useful for figuring out where to focus our efforts to keep customers happy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd225a0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
